# MapReduce step
***
1. 입력
   - 데이터를 입력하는 단계
   - 텍스트, csv, gzip 형태의 데이터를 읽어서 맵으로 전달
2. 맵(Map)
   - 입력을 분할하여 키별로 데이터를 처리
3. 컴바이너(Combiner)
   - 네트워크를 타고 넘어가는 데이터를 줄이기 위하여 맵의 결과를 정리
   - 로컬 리듀서라고도 함
   - 컴바이너는 작업의 설정에 따라 없을 수도 있음
4. 파티셔너(Partitoner)
   - 맵의 출력 결과 키 값을 해쉬 처리하여 어떤 리듀서로 넘길지를 결정
5. 셔플(Shuffle)
   - 각 리듀서로 데이터 이동
6. 정렬(Sort)
   - 리듀서로 전달된 데이터를 키 값 기준으로 정렬
7. 리듀서(Reduce)
   - 리듀서로 데이터를 처리하고 결과를 저장
8. 출력
   - 리듀서의 결과를 정의된 형태로 저장
  
# 입력
* * *
## InputFormat
***
입력파일이 분할되는 방식(InputSplit)이나 읽어들이는 방식(RecordReader)을 정의하는 클래스 입니다. InputFormat 추상클래스를 상속하여 구현합니다.

하둡은 파일을 읽기 위한 FileInputFormat이나, 여러개의 파일을 한번에 읽을 수 있는 CombineFileInputFormat 등을 제공합니다.
```
package org.apache.hadoop.mapreduce;

@InterfaceAudience.Public
@InterfaceStability.Stable
public abstract class InputFormat<K, V> {

  /** 
   * Logically split the set of input files for the job.  
   */
  public abstract 
    List<InputSplit> getSplits(JobContext context
                               ) throws IOException, InterruptedException;

  /**
   * Create a record reader for a given split. The framework will call
   */
  public abstract 
    RecordReader<K,V> createRecordReader(InputSplit split,
                                         TaskAttemptContext context
                                        ) throws IOException, 
                                                 InterruptedException;

}
```

## InputSplit
***
InputSplit은 맵의 입력으로 들어오는 데이터를 분할 하는 방식을 제공합니다. 데이터의 위치와 읽어 들이는 길이를 정의합니다.

```
package org.apache.hadoop.mapreduce;

@InterfaceAudience.Public
@InterfaceStability.Stable
public abstract class InputSplit {
  /**
   * Get the size of the split, so that the input splits can be sorted by size.
   */
  public abstract long getLength() throws IOException, InterruptedException;

  /**
   * Get the list of nodes by name where the data for the split would be local.
   * The locations do not need to be serialized.
   */
  public abstract 
    String[] getLocations() throws IOException, InterruptedException;

  /**
   * Gets info about which nodes the input split is stored on and how it is
   * stored at each location.
   */
  @Evolving
  public SplitLocationInfo[] getLocationInfo() throws IOException {
    return null;
  }
}
```
## RecordReader
***
RecordReader는 실제 파일에 접근하여 데이터를 읽어 들입니다. 데이터를 읽어서 <키, 밸류> 형태로 반환합니다.

```
package org.apache.hadoop.mapreduce;

import java.io.Closeable;
import java.io.IOException;

import org.apache.hadoop.classification.InterfaceAudience;
import org.apache.hadoop.classification.InterfaceStability;

/**
 * The record reader breaks the data into key/value pairs for input to the
 */
@InterfaceAudience.Public
@InterfaceStability.Stable
public abstract class RecordReader<KEYIN, VALUEIN> implements Closeable {

  /**
   * Called once at initialization.
   */
  public abstract void initialize(InputSplit split,
                                  TaskAttemptContext context
                                  ) throws IOException, InterruptedException;

  /**
   * Read the next key, value pair.
   */
  public abstract 
  boolean nextKeyValue() throws IOException, InterruptedException;

  /**
   * Get the current key
   */
  public abstract
  KEYIN getCurrentKey() throws IOException, InterruptedException;

  /**
   * Get the current value.
   */
  public abstract 
  VALUEIN getCurrentValue() throws IOException, InterruptedException;

  /**
   * The current progress of the record reader through its data.
   */
  public abstract float getProgress() throws IOException, InterruptedException;

  /**
   * Close the record reader.
   */
  public abstract void close() throws IOException;
}
```

## Mapper
***
매퍼는 사용자가 정의한 작업을 수행합니다. 사용자는 Mapper 클래스를 상속하고 map() 메소드를 구현하면 됩니다.

run() 메소드를 보면 실제 매퍼 작업이 동작하는 방식을 알 수 있습니다. setup() 메소드로 매퍼를 초기화하고, RecordReader를 이용하여 데이터를 읽어서 map(키, 밸류) 함수를 호출합니다. 데이터를 모두 처리할 때까지 반복합니다. 마지막으로 cleanup() 메소드로 사용한 리소스의 반환 등을 처리합니다

```
package org.apache.hadoop.mapreduce;

@InterfaceAudience.Public
@InterfaceStability.Stable
public class Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {

    /**
     * The <code>Context</code> passed on to the {@link Mapper} implementations.
     */
    public abstract class Context implements MapContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {
    }

    /**
     * Called once at the beginning of the task.
     */
    protected void setup(Context context) throws IOException, InterruptedException {
        // NOTHING
    }

    /**
     * Called once for each key/value pair in the input split. Most applications
     * should override this, but the default is the identity function.
     */
    @SuppressWarnings("unchecked")
    protected void map(KEYIN key, VALUEIN value, Context context) throws IOException, InterruptedException {
        context.write((KEYOUT) key, (VALUEOUT) value);
    }

    /**
     * Called once at the end of the task.
     */
    protected void cleanup(Context context) throws IOException, InterruptedException {
        // NOTHING
    }

    /**
     * Expert users can override this method for more complete control over the
     * execution of the Mapper.
     */
    public void run(Context context) throws IOException, InterruptedException {
        setup(context);
        try {
            while (context.nextKeyValue()) {
                map(context.getCurrentKey(), context.getCurrentValue(), context);
            }
        } finally {
            cleanup(context);
        }
    }
}
```

## Combiner
***
맵리듀스 프레임워크의 자원은 유한하기 때문에 맵 작업의 결과를 리듀스 작업의 입력으로 전달하기 전에 정리를 하면 데이터 전송에 필요한 자원을 줄일 수 있습니다. 이때 컴바이너 함수를 이용하여 처리할 수 있습니다.

하지만 컴바이너 작업에 사용할 수 있는 함수는 제약이 있습니다. 최대값, 최소값, 카운트 같은 함수는 가능하지만 평균 함수는 맵퍼 작업의 결과를 평균 내어, 리듀스 작업에 사용하면 최종 결과가 달라지기 때문에 사용 할 수 없습니다.

리듀스 함수를 컴바이너 함수로 사용할 수도 있지만, 컴바이너 함수를 리듀스 함수를 완전히 대체할 수도 없습니다. 맵 단계에서 컴바이너 함수를 쓰더라도 리듀스 함수는 서로 다른 맵에서 오는 같은 키의 레코드를 여전히 처리해야 합니다.

컴바이너를 사용하면 매퍼와 리듀서 사이의 셔플 단계에서 전송되는 데이터양을 줄일 수 있어서 작업의 속도를 향상 시킬 수 있습니다.

## Partitioner & Shuffle
***
맵 작업이 종료되면 작업이 끝난 노드부터 리듀서로 데이터를 전달 하는데 이를 셔플이라고 합니다. 리듀서로 데이터를 전달하기 위해서 맵의 결과 키를 리듀서로 분배하는 기준을 만드는 것을 파티션이라고 합니다.

기본적으로 사용되는 해쉬파티션을 보면 키의 값을 이용하여 리듀서의 개수만큼 파티션을 생성합니다. 같은 파티션은 같은 리듀서로 전달됩니다.

```
package org.apache.hadoop.mapreduce.lib.partition;

@InterfaceAudience.Public
@InterfaceStability.Stable
public class HashPartitioner<K, V> extends Partitioner<K, V> {

    /** Use {@link Object#hashCode()} to partition. */
    public int getPartition(K key, V value, int numReduceTasks) {
        return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;
    }

}
```
기본 파티셔너에서 Integer.MAX_VALUE값을 이용하여 논리합(&) 연산을 하는 이유는 파티션의 번호가 양수여야 하기 때문입니다. 자바의 int형의 첫번째 byte는 0이면 양수 이기 때문에 해쉬코드 연산을 통해서 음수가 나오면 논리합 연산을 통해 양수로 변경합니다. 따라서 사용자가 커스텀 파티셔너를 구현할 때 음수가 전달되지 않도록 잘 처리해 주어야 합니다.

## Sort
***
리듀스 작업전에 전달받은 키를 이용하여 정렬을 수행합니다. 그리고 실제 리듀스 작업을 수행하기 위해 데이터를 List< Value > 형태로 만들기 위한 그룹핑 작업도 함께 수행합니다.

리듀서의 키와 다른 값을 함께 이용하는 복합키의 경우 다음과 같이 파티셔닝, 소트, 그룹핑 작업을 거치면서 복합키를 기준으로 정렬하게 됩니다.

![sort](https://i.stack.imgur.com/l6IEl.png)

데이터를 정렬할 때 파티션의 기준이 되는 주키(Primary Key)외 다른 값을 기준으로 정렬할 수도 있습니다. 이를 세컨더리 소트(Secondary Sort)라고 합니다.

## Reduce
***
리듀서는 키별로 정렬된 데이터를 이용하여 리듀서 작업을 진행합니다.

리듀서는 사용자가 정의한 작업을 수행합니다. 사용자는 Reducer 클래스를 상속하고 reduce() 메소드를 구현하면 됩니다.

run() 메소드를 보면 실제 매퍼 작업이 동작하는 방식을 알 수 있습니다.

setup() 메소드로 리듀서를 초기화하고, 데이터를 읽어서 run(키, list(밸류)) 함수를 호출합니다. 데이터를 모두 처리할 때까지 반복합니다. 마지막으로 cleanup() 메소드로 사용한 리소스의 반환 등을 처리합니다

```
package org.apache.hadoop.mapreduce;

@Checkpointable
@InterfaceAudience.Public
@InterfaceStability.Stable
public class Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {

  /**
   * The <code>Context</code> passed on to the {@link Reducer} implementations.
   */
  public abstract class Context 
    implements ReduceContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
  }

  /**
   * Called once at the start of the task.
   */
  protected void setup(Context context
                       ) throws IOException, InterruptedException {
    // NOTHING
  }

  /**
   * This method is called once for each key. Most applications will define
   * their reduce class by overriding this method. The default implementation
   * is an identity function.
   */
  @SuppressWarnings("unchecked")
  protected void reduce(KEYIN key, Iterable<VALUEIN> values, Context context
                        ) throws IOException, InterruptedException {
    for(VALUEIN value: values) {
      context.write((KEYOUT) key, (VALUEOUT) value);
    }
  }

  /**
   * Called once at the end of the task.
   */
  protected void cleanup(Context context
                         ) throws IOException, InterruptedException {
    // NOTHING
  }

  /**
   * Advanced application writers can use the 
   * {@link #run(org.apache.hadoop.mapreduce.Reducer.Context)} method to
   * control how the reduce task works.
   */
  public void run(Context context) throws IOException, InterruptedException {
    setup(context);
    try {
      while (context.nextKey()) {
        reduce(context.getCurrentKey(), context.getValues(), context);
        // If a back up store is used, reset it
        Iterator<VALUEIN> iter = context.getValues().iterator();
        if(iter instanceof ReduceContext.ValueIterator) {
          ((ReduceContext.ValueIterator<VALUEIN>)iter).resetBackupStore();        
        }
      }
    } finally {
      cleanup(context);
    }
  }
}
```

# 출력
* * *

## OutputFormat
***
출력파일에 기록되는 형식을 설정합니다. Text 형식, Csv 형식 등을 설정하고 파일을 쓰는 방법을 설정합니다.

```
import java.io.IOException;

@InterfaceAudience.Public
@InterfaceStability.Stable
public abstract class OutputFormat<K, V> {

  public abstract RecordWriter<K, V> 
    getRecordWriter(TaskAttemptContext context
                    ) throws IOException, InterruptedException;

  public abstract void checkOutputSpecs(JobContext context
                                        ) throws IOException, 
                                                 InterruptedException;

  public abstract 
  OutputCommitter getOutputCommitter(TaskAttemptContext context
                                     ) throws IOException, InterruptedException;
}
```
