# 성능최적화
***
맵리듀스는 여러단계를 거치기 때문에 특정 설정하나로 성능을 높이기는 어렵습니다. 작업 속도가 느린 이유는 여러가지가 있을 수 때문에 다양한 방식으로 수정해 보는 것이 좋습니다.

# 매퍼, 리듀서 수 설정
***
매퍼수와 리듀서수에 따라 작업의 속도가 빨라질 수 있습니다. 매퍼 하나에 많은 파일이 몰리거나, 메모리를 많이 사용하는 작업이어서 GC에 많은 시간이 걸려서 그럴수 있습니다. 원천 데이터의 입력 사이즈에 따라 매퍼, 리듀서의 개수를 조절해 주는 것이 좋습니다.
```java
<property>
   <name>mapreduce.job.maps</name>
   <value>100</value>
</property>
<property>
   <name>mapreduce.job.reduces</name>
   <value>50</value>
 </property>
```
# 정렬 속성 튜닝(io.sort.* 튜닝)
***
맵 작업은 임시 결과 파일 개수를 줄이는 것으로 성능을 개선할 수 있다. 로컬 디스크에 저장된 파일이 줄어들수록 맵 출력 데이터의 병합, 네트워크 전송, 리듀서의 병합작업 시간이 단축됩니다. 스필되는 파일을 줄이려면 스필전 메모리 버퍼 크기인 io.sort.mb를 늘이면 됩니다. 메모리 버퍼가 커져서 로컬에 저장될 출력 데이터가 줄어들게 됩니다.
```java
  <property>
    <name>mapreduce.task.io.sort.mb</name>
    <value>200</value>
  </property>
  <property>
    <name>mapreduce.map.sort.spill.percent</name>
    <value>0.80</value>
  </property>
  <property>
    <name>mapreduce.task.io.sort.factor</name>
    <value>100</value>
  </property>
 ```
 
# 컴바이너 클래스 적용
***
컴바이너를 적용하면 맵 작업의 결과 데이터가 리듀서로 전송되기 전에 컴바이너 작업을 진행하여, 데이터를 줄여서 네트워크 사용량을 줄이고 리듀서의 작업 속도를 향상 시킬 수 있습니다.
```java
job.setMapperClass(TokenizerMapper.class); 
job.setCombinerClass(IntSumReducer.class); // 콤바이너 적용 
job.setReducerClass(IntSumReducer.class);
```

# 맵출력 데이터 압축
***
맵 출력 데이터를 압축하여 네트워크 트래픽을 줄여 주면 플레인 텍스트를 이용할 때보다 속도가 증가 할 수 있습니다.
```java
  <property>
    <name>mapreduce.map.output.compress</name>
    <value>true</value>
  </property>
  <property>
    <name>mapreduce.map.output.compress.codec</name>
    <value>org.apache.hadoop.io.compress.SnappyCodec</value>
  </property>
```
# 작은 파일 문제(small file problem) 수정
***
네임노드는 파일의 메타데이터와 블록을 관리하는 데 많은 메모리를 사용합니다. 작은 사이즈의 파일이 여러개 존재하게 되면 이 파일들을 관리하는데 많은 메모리가 사용되고, 맵리듀스 작업 처리중 많은 요청을 처리하기 되어 네임노드에 병목현상이 발생하게 되어 작업 속도가 느려지게 됩니다. 이를 방지하기 위해서 작은 사이즈의 파일을 하나의 파일로 합쳐서 HDFS 블록사이즈 크기의 파일로 설정하는 것이 좋습니다.

주요 해결방법은 작은 사이즈의 파일을 합쳐서 하나의 파일로 생성하는 것입니다. 이를 위해 하둡은 har 파일을 생성하는 방법을 제공합니다. 다른 방법은 작은 파일을 묶어서 하나의 압축 파일로 생성하는 방법입니다.
    
- [hadoop archives](https://hadoop.apache.org/docs/stable/hadoop-archives/HadoopArchives.html)
- [hr-files-hadoop-archive-files](http://hadooptutorial.info/har-files-hadoop-archive-files/)



